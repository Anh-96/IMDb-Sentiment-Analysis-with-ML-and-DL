# ğŸ¬ IMDb Sentiment Analysis with Machine Learning & Deep Learning

An end-to-end NLP project that compares traditional Machine Learning, Deep Learning, and Transformer-based models for sentiment classification on movie reviews.

This repository demonstrates the full workflow from raw text â†’ preprocessing â†’ feature engineering â†’ classical ML â†’ deep learning â†’ BERT fine-tuning â†’ model comparison.


ğŸ“Œ Project Overview

This project focuses on classifying IMDb movie reviews into:

* Positive ğŸ˜Š

* Negative ğŸ˜

and answering a practical question:

  How much better is BERT compared to traditional ML and Deep Learning models?

The goal is not only accuracy, but also understanding:

* When simple models are enough

* When deep learning helps

* When transformers truly shine


## ğŸ§  Models Implemented
ğŸŸ¢ Traditional ML

* TF-IDF + Logistic Regression (baseline)

* TF-IDF + SVM

* Hyperparameter tuning with GridSearch

ğŸ”µ Deep Learning

Neural Network baseline

CNN for text classification

LSTM / BiLSTM

ğŸ”´ Transformer

BERT fine-tuning for sentiment classification


## ğŸ“‚ Project Structure
00_install.ipynb              â†’ Environment setup
01_eda.ipynb                  â†’ Exploratory Data Analysis
02_text_cleaning.ipynb        â†’ Text preprocessing pipeline
03_tfidf_vectorization.ipynb  â†’ TF-IDF feature engineering
04_logistic_regression_baseline.ipynb
05A_model_tuning.ipynb        â†’ GridSearch tuning
05B_model_comparison.ipynb    â†’ ML model comparison
06_baseline_deep_learning.ipynb
07_cnn.ipynb                  â†’ CNN model
08_lstm_bilstm.ipynb          â†’ LSTM & BiLSTM
09_bert.ipynb                 â†’ BERT fine-tuning


## ğŸ” Workflow
1ï¸âƒ£ EDA

* Review length distribution

* Class balance

* Text statistics

2ï¸âƒ£ Text Cleaning

* Lowercasing

* Removing HTML tags

* Removing punctuation

* Stopword removal

* Lemmatization

3ï¸âƒ£ Feature Engineering

* TF-IDF (unigram & bigram)

* max_features tuning

4ï¸âƒ£ Classical ML Baselines

* Logistic Regression

* SVM

* GridSearch optimization

5ï¸âƒ£ Deep Learning Models

* Tokenization & padding

* Embedding layers

* CNN & LSTM architectures

6ï¸âƒ£ Transformer (BERT)

* Tokenization with pretrained model

* Fine-tuning on IMDb reviews

* Evaluation using F1 score


## ğŸ“Š Key Insights

* From experimentation across models:

* TF-IDF + Logistic Regression is a strong baseline

* CNN/LSTM improve performance but require more tuning

* BERT delivers the best contextual understanding

* Transformers outperform others on:

  - long reviews

  - mixed sentiment

  - ambiguous language

However:

* ML models train extremely fast âš¡

* BERT is computationally expensive ğŸ¢

So in real production:

  Simple models can still be very competitive.


## ğŸ“ˆ Evaluation Metrics

* Accuracy

* F1 Score (main metric)

* Precision / Recall

* Confusion Matrix

* Error analysis on misclassified samples

## ğŸ§ª Example Predictions

The models were tested on difficult, ambiguous reviews such as:

* "It wasnâ€™t bad, but I wouldnâ€™t watch it again."

* "Strangely enjoyable despite its flaws."

These cases highlight the advantage of contextual models like BERT.


## ğŸ› ï¸ Tech Stack

* Python

* Scikit-learn

* TensorFlow / Keras

* PyTorch

* HuggingFace Transformers

* Pandas / NumPy / Matplotlib


## ğŸ¯ What This Project Demonstrates

This project showcases:

* End-to-end NLP pipeline design

* Feature engineering for text data

* Model comparison methodology

* Hyperparameter tuning

* Deep learning for NLP

* Transformer fine-tuning

* Error analysis mindset


## ğŸš€ Possible Improvements

Future directions:

* DistilBERT (faster alternative to BERT)

* RoBERTa fine-tuning

* Ensemble models

* Deployment as an API

* Real-time sentiment prediction UI


## ğŸ“ Dataset

IMDb movie reviews dataset:

* ~50,000 labeled reviews

* Balanced positive/negative classes

* Widely used NLP benchmark


## ğŸ‘¨â€ğŸ’» Author

Built as a hands-on learning project to explore:

* Machine Learning

* Deep Learning

* NLP pipelines

* Transformer models
